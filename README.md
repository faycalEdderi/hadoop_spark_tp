ğŸš€ Projet Big Data â€” Analyse des cryptomonnaies via Spark, HDFS & MongoDB

> Pipeline complet :
> Binance â†’ Python â†’ HDFS â†’ Spark â†’ MongoDB â†’ Express â†’ React
>
> Visualisation des prix, volatilitÃ©, et comparaison entre cryptos "solid" (BTC, ETH) et "meme" (DOGE, SHIB, PEPE, etc.)

ğŸ¯ Objectif du projet

Ce projet met en Å“uvre un pipeline de donnÃ©es complet dans un environnement Big Data DockerisÃ© :

1. RÃ©cupÃ©ration des prix en temps rÃ©el depuis lâ€™API Binance.
2. Stockage dans HDFS via un script Python.
3. Traitement avec Spark (agrÃ©gations, calculs de volatilitÃ©, classement).
4. Ã‰criture des rÃ©sultats dans MongoDB.
5. Exposition via une API Express.
6. Visualisation interactive avec React + Recharts.

ğŸ§© Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Binance  â”‚â”€â”€â”€â”€>â”‚ Python â”‚â”€â”€â”€â”€>â”‚  HDFS  â”‚â”€â”€â”€â”€>â”‚  Spark  â”‚â”€â”€â”€â”€>â”‚ MongoDB  â”‚<â”€â”€â”€>â”‚ Express â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                       â”‚
                                                                                       â–¼
                                                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                                 â”‚   React    â”‚
                                                                                 â”‚ Dashboard  â”‚
                                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ› ï¸ Technologies utilisÃ©es

- Python : Scripts de rÃ©cupÃ©ration et dâ€™ingestion
- Hadoop HDFS : Stockage distribuÃ© des donnÃ©es brutes
- Apache Spark : Traitement distribuÃ©, calculs statistiques
- MongoDB : Stockage des rÃ©sultats agrÃ©gÃ©s
- Express.js : API REST pour exposer les donnÃ©es
- React + Recharts : Dashboard de visualisation
- Docker & Docker Compose : Environnement conteneurisÃ© complet

ğŸ“‚ Structure du projet

â”œâ”€â”€ docker-compose.yml          # Orchestration des services
â”œâ”€â”€ app/                        # Scripts Spark
â”‚   â”œâ”€â”€ binance_spark.py        # Collecte + traitement Spark â†’ MongoDB
â”‚   â””â”€â”€ hdfs_to_mongo.py        # Lecture HDFS â†’ Ã©criture MongoDB
â”œâ”€â”€ myhadoop/                   # Scripts Hadoop/Python
â”‚   â”œâ”€â”€ binance_to_hdfs.py      # RÃ©cupÃ¨re Binance â†’ Ã©crit CSV â†’ envoie dans HDFS
â”‚   â””â”€â”€ ...
â”œâ”€â”€ backend/                    # API Express
â”‚   â”œâ”€â”€ server.js               # Routes vers MongoDB
â”‚   â””â”€â”€ .env                    # Config MongoDB
â”œâ”€â”€ frontend/                   # Dashboard React
â”‚   â””â”€â”€ src/App.js              # Visualisations (tableaux, graphiques)
â””â”€â”€ README.txt                  # Ce fichier

âš™ï¸ PrÃ©requis

- Docker + Docker Compose
- AccÃ¨s Internet (API Binance)
- Navigateur moderne (pour le frontend)

ğŸš€ Lancer le projet

1. DÃ©marrer lâ€™environnement

docker-compose up -d

Services dÃ©marrÃ©s :
- namenode, datanode, resourcemanager, nodemanager â†’ Hadoop
- spark-master, spark-worker-1 â†’ Spark
- mongo â†’ MongoDB
- (Backend et frontend Ã  lancer manuellement â€” voir plus bas)

2. Ingestion des donnÃ©es â†’ HDFS

# Copier le script dans le conteneur
docker cp ./myhadoop/binance_to_hdfs.py namenode:/myhadoop/

# Lancer le script (installe Python 3.5 si nÃ©cessaire)
docker exec namenode python3 /myhadoop/binance_to_hdfs.py

# Copier-coller la commande HDFS affichÃ©e, ex:
docker exec namenode hdfs dfs -mkdir -p /usr/binance && hdfs dfs -put -f /tmp/binance_prices.csv /usr/binance/

3. Traitement Spark â†’ MongoDB

# Script 1 : DonnÃ©es directes Binance â†’ MongoDB
docker cp ./app/binance_spark.py spark-master:/app/
docker exec -it spark-master /spark/bin/spark-submit \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 \
  /app/binance_spark.py

# Script 2 : DonnÃ©es HDFS â†’ MongoDB (preuve du pipeline)
docker cp ./app/hdfs_to_mongo.py spark-master:/app/
docker exec -it spark-master /spark/bin/spark-submit \
  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 \
  /app/hdfs_to_mongo.py

4. Lancer le backend (Express)

cd backend
npm install
npm start
# â†’ Ã‰coute sur http://localhost:5000

5. Lancer le frontend (React)

cd frontend
npm install
npm start
# â†’ Ouvre automatiquement http://localhost:3000

ğŸ“Š FonctionnalitÃ©s du Dashboard

- âœ… Tableau des prix moyens, min, max par crypto
- âœ… Comparaison catÃ©gorielle (solid vs meme)
- âœ… Classement par amplitude de prix
- âœ… Graphique de volatilitÃ© par catÃ©gorie
- âœ… Scatter plots avancÃ©s (prix moyen vs amplitude)
- âœ… NOUVEAU : Tableau des donnÃ©es provenant de HDFS â†’ preuve du pipeline complet

ğŸ§ª Collections MongoDB gÃ©nÃ©rÃ©es

| Collection            | Description                                  |
|-----------------------|----------------------------------------------|
| prices                | DonnÃ©es brutes depuis Binance                |
| summary               | Statistiques par symbole (avg/min/max)       |
| ranking               | Classement par amplitude de prix             |
| category_summary      | Statistiques par catÃ©gorie (solid/meme)      |
| volatility            | Ã‰cart-type des variations par catÃ©gorie      |
| prices_from_hdfs      | DonnÃ©es chargÃ©es depuis HDFS â†’ preuve du pipeline |

ğŸ’¡ IdÃ©es dâ€™amÃ©lioration

- Ajouter un scheduler (cron) pour la collecte automatique
- IntÃ©grer Kafka + Spark Streaming pour du temps rÃ©el
- Ajouter des alertes (prix seuil, volatilitÃ© anormale)
- Export CSV/PDF des rapports
- Dark mode / thÃ¨mes personnalisables


Projet rÃ©alisÃ© dans le cadre du cours Hadoop/Spark â€” IPSSI

ğŸ“„ Licence

Projet Ã©ducatif â€” Libre dâ€™utilisation pour lâ€™apprentissage.